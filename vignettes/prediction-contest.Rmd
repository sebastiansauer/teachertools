---
title: "prediction-contest"
subittle: Vignette
output: 
  rmarkdown::html_vignette:
    toc: TRUE
    number_sections: TRUE
vignette: >
  %\VignetteIndexEntry{prediction-contest}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Setup

```{r setup}
library(teachertools)
library(tidyverse)  # data wrangling
```

# Set constants

## Specify Submission Data


In this example, we will make use of some demo data within this package.



```{r}
submission_folder_path <- system.file("extdata", package = "teachertools")

submission_files_path <- list.files(submission_folder_path, full.names = TRUE)

submission_files_path
```



Let's check the content of one of those files:

```{r}
one_submission <-
  read_csv(submission_files_path[1])

glimpse(one_submission)
```


In a normal case, you would provide a vector to your submission files, e.g., like this:

```{r eval= FALSE}
submission_files_path <- list.files(path = path_to_csv_submissions,
                                    full.names = TRUE,
                                    pattern = ".csv$|.CSV$|.Csv$|.CSv$|.csV$|.cSV$|.cSV$",
                                    recursive = TRUE) 
```

## Specify Train and Test Data


```{r}
d_train_path <- "https://raw.githubusercontent.com/sebastiansauer/pradadata/master/data-raw/ames_de_train.csv"
d_test_path <- "https://raw.githubusercontent.com/sebastiansauer/pradadata/master/data-raw/ames_de_test.csv"
```

Note that `file.exists()` is unable to check the existence of a remote file.
However, you may check by hand or use e.g, [this approach](https://stackoverflow.com/questions/30236518/check-the-existence-of-remote-directory-with-r).



# Compute Prediction Error per Submission


```{r}
d_error <- comp_error_submissions(
  path_to_submissions = submission_folder_path,
  name_output_var = "preis",
  path_to_test_data = d_test_path,
  path_to_train_data = d_train_path,
  error_fun = yardstick::mae,
  verbose = TRUE,
  start_id = 801)
```


Let's have a look:

```{r}
d_error %>% attributes()
glimpse(d_error)
```


Note that `id` refers to the student id, not the prediction id.



# Closer Look to the submissions


At times, student to not submit the correct column names, so 
that automatic parsing faces difficulties:

```{r}
d_error$colnames_pred_file
```


At times, students to not submit the correct number of predictions:


```{r}
d_error$npreds
```



# Grade the students

Importantly, we need thresholds defining the minimal performance for a given grade:

```{r}
grade_thresholds <-
  tibble::tribble(
    ~threshold, ~grade,
             0,    0.7,
        14250L,      1,
        15000L,    1.3,
        16000L,    1.7,
        17000L,      2,
        18100L,    2.3,
        18500L,    2.7,
        19000L,      3,
        20000L,    3.3,
        21000L,    3.7,
        22000L,      4,
        Inf,         5
  )

grade_thresholds %>% 
  gt::gt()
```


Your students will be quite interested in this table :-)



```{r finalize-grades}
d_grades <- finalize_grades(d_error = d_error,
                            thresholds = grade_thresholds$threshold)
```


```{r gt-dgrades}
gt::gt(d_grades)
```



# Plot distribution


```{r plot-grade-distro, out.width="50%", fig.align="center"}
plot_grade_distribution(d_grades)
```



# Prepare the CSV file

You normally need not call this function,
its job is to prepare the CSV file for computing the individual prediction errors.
This is neccessary as some CSV files contain glitches such as different delimiters,
or too many/few columns with strange names etc, all things that need consideration.
`prep_csv` tries to take care of those details.


The parameters are mostly identical to `compute_error_submissions`:

```{r}
data.table::fread(submission_files_path[1])
```



```{r}
d_prepred_csv <- 
  prep_csv(
    submission_file = "Lastname7_Firstname7_72345678_Prognose.csv",
    path_to_submissions = submission_folder_path,
    name_output_var = "preis",
    path_to_test_data = d_test_path,
    path_to_train_data = d_train_path,
    verbose = TRUE,
    start_id = 801
)
```


```{r}
d_prepred_csv %>% 
  head() %>% 
  gt::gt()
```




# Deal with missing students

Sometimes there are student submissions for which you, as the teacher, will not find a grade,
after running these `teachertools`. 
Let's call those cases "submitted-not-graded".
Similarly, in some cases, there will be students for which you have a grade,
but the students failed to properly enroll to the exam.
Let's call those cases "submitted-not-enrolled".
Finally, often times, some students do enroll, but for some reason,
fail to submit their exam.
Let's call those cases "enrolled-not-submitted".
It makes sense to check for all three cases,
in order to be sure that your grading procedure is not to blame for any of 
the three above mentioned inconsistencies.
Let's look at each of the three inconsistencies in turn.


## Submitted-not-graded


Get a list of all submitted exam files (here without path):

```{r}
submission_files_wo_path <- list.files(submission_folder_path, 
                                       full.names = FALSE)
submission_files_wo_path
```

Strip the student id (assumed to be more reliabel compared to the name).
Some Regex is of help, in the case of the example file names above,
the ID number can be stripped like this:


```{r}
subm_ids <-
  submission_files_wo_path %>% 
  map_chr(parse_matrikelnummer) 

subm_ids
```


Now we can compare the two sets of submitted (`subm_ids`) and processed with a grade (`d_grade$id`):

```{r}
setdiff(subm_ids, d_grades$id)
```


Remember:

```{r}
x <- c(1, 2, 3)
y <- c(1)

setdiff(x, y)
```


In other words, `setdiff()` checks *which elements from `x` are not in `y`*.

In our case, we check which elements of the submitted exams `subm_id` are not part of the graded ones `d_grades`.


## Submitted-not-enrolled

Get a list of the students who formally enrolled to your course.

Let's assume this is the list:

```{r}
enrolled_students <- c("22345678", "32345678", "42345678", "52345678", "62345678", "72345678", "82345678")
```

Again, we can compare the two relevant sets: Which elements from the set of enrolled students is not part of the set of grades students?

```{r}
setdiff(enrolled_students, d_grades$id)
```


## Enrolled-not-submitted

Vice versa, let's check which students ("elements") submitted their exam but are not formally enrolled to the exam?


```{r}
setdiff(d_grades$id, enrolled_students)
```

